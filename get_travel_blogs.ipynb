{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da741aa3",
   "metadata": {},
   "source": [
    "# Getting travel blog content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e624bb79",
   "metadata": {},
   "source": [
    "1. Get travel blog urls with SERP API\n",
    "2. Get all wordpress pages from original blog url\n",
    "3. Get content from pages\n",
    "4. Get metadata from pages\n",
    "5. Use content to get location name with spacy's \"en_core_web_sm\"\n",
    "6. Use geopy's Nominatim to get lat and long from location name\n",
    "7. Write to DynamoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf7c4688",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/merho/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings\n",
    "from serpapi import GoogleSearch\n",
    "from backend.off_the_path.src.utilities import clean_text\n",
    "import numpy as np\n",
    "import spacy\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.exc import GeocoderTimedOut\n",
    "from urllib.parse import urlparse\n",
    "import pandas as pd\n",
    "import boto3\n",
    "\n",
    "# Load a pre-trained English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Load the geolocator\n",
    "geolocator = Nominatim(user_agent=\"off_the_path\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a39046",
   "metadata": {},
   "source": [
    "Get list of travel sites from google.\n",
    "\n",
    "Query: \"travel site:wordpress.com\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ae13bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serpapi_search(query, api_key):\n",
    "    params = {\n",
    "        \"api_key\": api_key,\n",
    "        \"engine\": \"google\",\n",
    "        \"q\": query,\n",
    "        \"hl\": \"en\"\n",
    "    }\n",
    "    search = GoogleSearch(params)\n",
    "    results = search.get_dict()\n",
    "    return results.get('organic_results', [])\n",
    "\n",
    "query = \"travel site:wordpress.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be63cf8",
   "metadata": {},
   "source": [
    "Get all page links from base blog site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77638cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordpress_pages(base_url):\n",
    "    # init list to store page links\n",
    "    all_pages = []\n",
    "    # List of sitemap URL suffixes to try\n",
    "    sitemap_paths = [\"post-sitemap.xml\", \"sitemap-1.xml\", \"sitemap.xml\", \"sitemap_index.xml\",]\n",
    "    headers = {\n",
    "        \"User-Agent\": (\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "            \"Chrome/126.0 Safari/537.36\"\n",
    "        ),\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Referer\": \"https://www.google.com/\"\n",
    "    }\n",
    "    \n",
    "    # Try each sitemap path until a valid one is found\n",
    "    sitemap_url = None\n",
    "    for path in sitemap_paths:\n",
    "        try:\n",
    "            test_url = base_url.rstrip('/') + '/' + path\n",
    "            r = requests.get(test_url, headers=headers, timeout=10)\n",
    "            if 200 <= r.status_code < 300:\n",
    "                sitemap_url = test_url\n",
    "                sitemap_response = r\n",
    "                break\n",
    "        except requests.RequestException:\n",
    "            continue \n",
    "\n",
    "    if not sitemap_url:\n",
    "        print(\"No valid sitemap found.\")\n",
    "        return []\n",
    "\n",
    "    # xml parser didnt work so html is a must, but it throws a warning\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "        soup = BeautifulSoup(sitemap_response.text, 'html.parser')\n",
    "    # find all links on page and store in a list\n",
    "    links = soup.find_all('url')\n",
    "    for l in links:\n",
    "        loc = l.find('loc')\n",
    "        if loc:\n",
    "            all_pages.append(loc.text)\n",
    "    return all_pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e623657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_useful_url(url):\n",
    "\n",
    "    parsed = urlparse(url)\n",
    "    path = parsed.path.lower()\n",
    "\n",
    "    # skip pagination\n",
    "    if \"/page/\" in path:\n",
    "        return False\n",
    "\n",
    "    # skip categories, tags, authors\n",
    "    skip_segments = [\"category\", \"tag\", \"author\"]\n",
    "    if any(f\"/{seg}/\" in path for seg in skip_segments):\n",
    "        return False\n",
    "\n",
    "    # skip feeds\n",
    "    if \"/feed/\" in path:\n",
    "        return False\n",
    "\n",
    "    # skip wp system URLs\n",
    "    if \"wp-json\" in path or \"wp-admin\" in path:\n",
    "        return False\n",
    "\n",
    "    # skip attachments / misc\n",
    "    bad_ext = (\".jpg\", \".png\", \".gif\", \".pdf\", \".xml\", \".zip\")\n",
    "    if path.endswith(bad_ext):\n",
    "        return False\n",
    "\n",
    "    # skip query-string pages entirely\n",
    "    if parsed.query:\n",
    "        return False\n",
    "\n",
    "    # optional: only allow URLs that “look like” posts/pages\n",
    "    # e.g., end in a slash and contain a slug\n",
    "    if not path.endswith(\"/\"):\n",
    "        return False\n",
    "\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43ae2e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blog_page_content(page_url):\n",
    "    headers = {\n",
    "        \"User-Agent\": (\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "            \"Chrome/126.0 Safari/537.36\"\n",
    "        ),\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Referer\": \"https://www.google.com/\"\n",
    "    }\n",
    "    response = requests.get(page_url, headers=headers, timeout=15)\n",
    "\n",
    "    # Parse the HTML with Beautiful Soup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    paragraphs = soup.find_all('p')\n",
    "    all_paras = \" \".join(paragraphs)\n",
    "    return all_paras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ad5dcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blog_page_meta_data(page_url):\n",
    "    headers = {\n",
    "        \"User-Agent\": (\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "            \"Chrome/126.0 Safari/537.36\"\n",
    "        ),\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Referer\": \"https://www.google.com/\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(page_url, headers=headers, timeout=15)\n",
    "\n",
    "    # Parse the HTML with Beautiful Soup\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    title_list = soup.find_all('title')\n",
    "    title = str(title_list[0])\n",
    "    title = title.replace(\"<title>\",\"\").replace(\"</title>\",\"\")\n",
    "\n",
    "    description_meta = soup.find('meta', attrs={'property': 'og:description'})\n",
    "    if description_meta:\n",
    "        description_content = description_meta.get('content')\n",
    "    else:\n",
    "        description_content = np.nan\n",
    "\n",
    "    author_meta = soup.find('meta', attrs={'property': 'author'})\n",
    "    if author_meta:\n",
    "        author_content = author_meta.get('content')\n",
    "    else:\n",
    "        author_content = np.nan\n",
    "\n",
    "    return title, description_content, author_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "102624f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_geo_name(title, description):\n",
    "\n",
    "    doc = nlp(title)\n",
    "\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"GPE\" or ent.label_ == \"LOC\": # GPE: Geopolitical Entity, LOC: Location\n",
    "            return ent\n",
    "        else:\n",
    "            doc = nlp(description)\n",
    "            for ent in doc.ents:\n",
    "                if ent.label_ == \"GPE\" or ent.label_ == \"LOC\":\n",
    "                    return ent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78f6a65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lat_long(location_name):\n",
    "    try:\n",
    "        location = geolocator.geocode(location_name)\n",
    "        if location:\n",
    "            lat = location.latitude\n",
    "            long = location.longitude\n",
    "            return lat, long\n",
    "        else:\n",
    "            print(f\"Could not find location for: {location_name}\")\n",
    "    except GeocoderTimedOut:\n",
    "        print(\"Error: Geocoding service timed out.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95c5a33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_urls = ['https://thetravellush.wordpress.com', 'https://dangerousbusiness.wordpress.com', 'https://ashleighbugg.wordpress.com']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4d302b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://thetravellush.wordpress.com\n",
      "Not enough info in https://travel-lush.com/blog/\n",
      "Not enough info in https://travel-lush.com/the-reality-of-traveling-to-a-disaster-zone-coron-philippines/\n",
      "Not enough info in https://travel-lush.com/backpacking-in-bogota-is-colombias-capital-worth-a-visit/\n",
      "Not enough info in https://travel-lush.com/nitty-gritty-behind-finding-job-indonesia/\n",
      "Not enough info in https://travel-lush.com/backpacker-expat-2014-travel/\n",
      "Not enough info in https://travel-lush.com/sampling-phnom-penhs-food-scene/\n",
      "Not enough info in https://travel-lush.com/one-year-blogiversary/\n",
      "Not enough info in https://travel-lush.com/living-jakarta-6-months/\n",
      "Not enough info in https://travel-lush.com/making-time-jakarta/\n",
      "Not enough info in https://travel-lush.com/christmas-jakarta-just-not/\n",
      "Not enough info in https://travel-lush.com/epic-quest-find-best-veggie-burger-jakarta/\n",
      "Not enough info in https://travel-lush.com/life-in-jakarta-my-taxi-ride-from-hell/\n",
      "Not enough info in https://travel-lush.com/moving-to-jakarta-10-things-to-pack/\n",
      "Not enough info in https://travel-lush.com/11-weird-habits-ive-picked-up-since-moving-to-indonesia/\n",
      "Not enough info in https://travel-lush.com/my-summer-travel-plans/\n",
      "Not enough info in https://travel-lush.com/love-affair-cambodia/\n",
      "Not enough info in https://travel-lush.com/the-many-sides-of-bali-in-photos/\n",
      "Not enough info in https://travel-lush.com/this-expat-life-month-10/\n",
      "Not enough info in https://travel-lush.com/tips-on-moving-abroad-and-how-you-can-learn-from-my-mistakes/\n",
      "Not enough info in https://travel-lush.com/why-i-love-phnom-penh/\n",
      "Not enough info in https://travel-lush.com/motorbiking-bohol-exploring-natural-wonders-natural-disasters/\n",
      "Not enough info in https://travel-lush.com/11-ways-living-abroad-changed-my-life/\n",
      "Not enough info in https://travel-lush.com/this-expat-life-month-11/\n",
      "Not enough info in https://travel-lush.com/returning-home-after-living-abroad/\n",
      "Not enough info in https://travel-lush.com/on-living-in-jakarta/\n",
      "Not enough info in https://travel-lush.com/this-expat-life-month-12/\n",
      "Not enough info in https://travel-lush.com/its-official-i-live-in-phnom-penh/\n",
      "Not enough info in https://travel-lush.com/what-to-pack-moving-to-southeast-asia/\n",
      "Not enough info in https://travel-lush.com/boracay-on-budget-backpaking-on-40-day/\n",
      "Not enough info in https://travel-lush.com/a-backpackers-debate-southeast-asia-vs-south-america-2/\n",
      "Not enough info in https://travel-lush.com/favorite-mexican-food-restaurants-san-diego/\n",
      "Not enough info in https://travel-lush.com/stepping-outside-of-my-comfort-zone-tbex/\n",
      "Not enough info in https://travel-lush.com/introverts-guide-tbex-asia/\n",
      "Not enough info in https://travel-lush.com/living-abroad-cambodia-southeast-asia/\n",
      "Not enough info in https://travel-lush.com/this-expat-life-month-14/\n",
      "Not enough info in https://travel-lush.com/this-expat-life-month-13/\n",
      "Not enough info in https://travel-lush.com/this-expat-life-month-16/\n",
      "Not enough info in https://travel-lush.com/looking-back-2015-travel-expat-living/\n",
      "Not enough info in https://travel-lush.com/this-expat-life-month-17/\n",
      "Not enough info in https://travel-lush.com/this-expat-life-month-18/\n",
      "Not enough info in https://travel-lush.com/im-moving-to-china/\n",
      "Not enough info in https://travel-lush.com/beijing-not-so-scary-after-all/\n",
      "Not enough info in https://travel-lush.com/this-expat-life-month-19-beijing/\n",
      "Not enough info in https://travel-lush.com/happy-blogiversary-travel-lush-turns-two/\n",
      "Not enough info in https://travel-lush.com/this-expat-life-beijing-month-20/\n",
      "Not enough info in https://travel-lush.com/expat-life-month-21-beijing/\n",
      "Not enough info in https://travel-lush.com/36-hours-in-hong-kong/\n",
      "Not enough info in https://travel-lush.com/living-beijing-month-22/\n",
      "Not enough info in https://travel-lush.com/living-beijing-month-23/\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## init dynamo DB connection\n",
    "# dynamodb = boto3.resource('dynamodb', region_name='us-east-1') \n",
    "# table = dynamodb.Table('off-the-beaten-path-blog-posts')\n",
    "\n",
    "\n",
    "id = 1\n",
    "for blog_url in query_urls:\n",
    "    # init empt dataframe\n",
    "    df = pd.DataFrame(columns=[\"id_num\", \"blog_url\",  \"page_url\",\"page_title\", \"page_description\",\n",
    "                   \"page_author\",\"content\",\"location_name\", \"latitude\",\"longitude\"])\n",
    "    print(blog_url)\n",
    "    all_links = get_wordpress_pages(blog_url)\n",
    "    for link in all_links:\n",
    "        if not is_useful_url(link):\n",
    "            print(\"Skipping:\", link)\n",
    "            continue\n",
    "        try:\n",
    "            # get blog content\n",
    "            content = get_blog_page_content(link)\n",
    "            # clean text\n",
    "            clean_content = clean_text(content)\n",
    "            # get blog meta data\n",
    "            title, description, author =  get_blog_page_meta_data(link)\n",
    "            place_name = find_geo_name(title, description)\n",
    "            lat, long = get_lat_long(place_name)\n",
    "            id_num = id.zfill(6)\n",
    "            if title:\n",
    "                new_row = {\"id_num\":id_num, \"blog_url\":blog_url,  \"page_url\":link,\"page_title\":title, \n",
    "                           \"page_description\": description, \"page_author\":author,\n",
    "                           \"content\":clean_content,\"location_name\":place_name, \n",
    "                           \"latitude\":lat ,\"longitude\": long}\n",
    "                df = df.append(new_row, ignore_index=True)\n",
    "                id +=1\n",
    "        except:\n",
    "            print(\"Not enough info in\",link)\n",
    "            continue\n",
    "    print(len(df))\n",
    "    print(df.head(5))\n",
    "    # # convert to list of dictionaries\n",
    "    # result = df.to_dict(orient='records')\n",
    "    # # add items to dynamoDB\n",
    "    # with table.batch_writer() as batch:\n",
    "    #     for item in result:\n",
    "    #         batch.put_item(Item=item)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
